{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nn import MotifCaller, NaiveCaller\n",
    "from training_data import data_preproc, load_training_data\n",
    "from utils import get_savepaths\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from greedy_decoder import GreedyCTCDecoder\n",
    "from Levenshtein import ratio\n",
    "from beam_search_decoder import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 17\n",
    "model_path = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\models\\synthetic\\local_trained.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_int = np.arange(n_classes).tolist()\n",
    "labels = [f\"{i}\" for i in labels_int] # Tokens to be fed into greedy decoder\n",
    "greedy_decoder = GreedyCTCDecoder(labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Loading model purely for inference\n",
    "    Will need to lead optimizer to fine tune\n",
    "    \"\"\"\n",
    "    # Model Definition\n",
    "    model = NaiveCaller(num_classes=17)\n",
    "    \n",
    "    if device == torch.device('cpu'):\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "    \n",
    "\n",
    "# Load model\n",
    "# device\n",
    "# port these useful methods to utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path=model_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_savepaths(running_on_hpc=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>motif_seq</th>\n",
       "      <th>base_seq</th>\n",
       "      <th>squiggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 11, 11, 7, 11, 11, 8, 11, 12, 2, 12, 12, 5...</td>\n",
       "      <td>TTTATCGTCGTCACATCAGTCGACATCAGTCGGCATGAAGACACTA...</td>\n",
       "      <td>[504, 506, 491, 502, 504, 515, 458, 472, 458, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14, 14, 8, 14, 15, 2, 15, 15, 5, 15, 15, 7, 1...</td>\n",
       "      <td>TGACGTCGGATGACGTCGGCAGCGCCACCAACTCCACAAATGACGT...</td>\n",
       "      <td>[469, 480, 479, 479, 481, 482, 483, 498, 505, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10, 10, 3, 10, 10, 7, 10, 11, 2, 11, 11, 4, 1...</td>\n",
       "      <td>GGACAGCTAGGGACAGCTACACCCCCGTATTTTGAGCGGGGGACAG...</td>\n",
       "      <td>[465, 470, 479, 486, 539, 532, 533, 525, 527, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[12, 1, 12, 12, 4, 12, 12, 6, 12, 12, 7, 12, 1...</td>\n",
       "      <td>TCGCCTTCATACCCCACTAACGTAGAGTACTGCCCTTCATACCCCT...</td>\n",
       "      <td>[485, 487, 496, 495, 488, 488, 490, 488, 524, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 12, 3, 12, 12, 4, 12, 12, 6, 12, 12, 7, 1...</td>\n",
       "      <td>AGTCGCCTTCATACCCACCCCCGTATTTTGAGCGGCCTTCATACCC...</td>\n",
       "      <td>[544, 556, 539, 547, 536, 534, 538, 493, 488, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           motif_seq  \\\n",
       "0  [5, 11, 11, 7, 11, 11, 8, 11, 12, 2, 12, 12, 5...   \n",
       "1  [14, 14, 8, 14, 15, 2, 15, 15, 5, 15, 15, 7, 1...   \n",
       "2  [10, 10, 3, 10, 10, 7, 10, 11, 2, 11, 11, 4, 1...   \n",
       "3  [12, 1, 12, 12, 4, 12, 12, 6, 12, 12, 7, 12, 1...   \n",
       "4  [11, 12, 3, 12, 12, 4, 12, 12, 6, 12, 12, 7, 1...   \n",
       "\n",
       "                                            base_seq  \\\n",
       "0  TTTATCGTCGTCACATCAGTCGACATCAGTCGGCATGAAGACACTA...   \n",
       "1  TGACGTCGGATGACGTCGGCAGCGCCACCAACTCCACAAATGACGT...   \n",
       "2  GGACAGCTAGGGACAGCTACACCCCCGTATTTTGAGCGGGGGACAG...   \n",
       "3  TCGCCTTCATACCCCACTAACGTAGAGTACTGCCCTTCATACCCCT...   \n",
       "4  AGTCGCCTTCATACCCACCCCCGTATTTTGAGCGGCCTTCATACCC...   \n",
       "\n",
       "                                            squiggle  \n",
       "0  [504, 506, 491, 502, 504, 515, 458, 472, 458, ...  \n",
       "1  [469, 480, 479, 479, 481, 482, 483, 498, 505, ...  \n",
       "2  [465, 470, 479, 486, 539, 532, 533, 525, 527, ...  \n",
       "3  [485, 487, 496, 495, 488, 488, 490, 488, 524, ...  \n",
       "4  [544, 556, 539, 547, 536, 534, 538, 493, 488, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['motif_seq', 'base_seq', 'squiggle'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset_path, model_save_path, file_write_path = get_savepaths(\n",
    "        running_on_hpc=False)\n",
    "\n",
    "X, y = load_training_data(\n",
    "        dataset_path, column_x='squiggle', column_y='motif_seq',\n",
    "        sampling_rate=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import torch.nn as nn\n",
    "\n",
    "ctc = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "def update_alignments(alignments, alignment_probs, top_n):\n",
    "\n",
    "    if len(alignments) == 0:\n",
    "        alignments.append(top_n)\n",
    "        alignment_probs.append(top_n_probs)\n",
    "        return alignments,\n",
    "    \n",
    "    for ind, alignment in enumerate(alignments):\n",
    "        last_char = alignment[-1]\n",
    "        for i, prob in zip(top_n, top_n_probs):\n",
    "            if i == last_char:  # If it's the same as before (whether blank or repeated char - it gets collapsed)\n",
    "                alignment_probs[ind] += prob\n",
    "\n",
    "            elif last_char == blank_index:  # If previous is a blank and this is a character, we can get rid of the previous blank\n",
    "                new_alignment = alignment[-1] + i\n",
    "                if new_alignment in alignments:\n",
    "                    alignments[alignments.index(new_alignment)] += prob\n",
    "                else:\n",
    "                    alignments.append(alignment[-1] + i)\n",
    "                    alignment_probs.append(alignment_probs[ind] + prob)\n",
    "\n",
    "            else:\n",
    "                alignments.append(alignment + i)\n",
    "                alignment_probs.append(alignment_probs[ind] + prob)\n",
    "                \n",
    "    # go through all the alignments and merge the leftover ones\n",
    "    # return the most probable one\n",
    "\n",
    "    \n",
    "\n",
    "def beam_search_ctc(prob_matrix, beam_width=3, blank=0, n_classes=17):\n",
    "    \n",
    "    # Get top n probabilities and their corresponding indices for each time step\n",
    "    # Create a list of alignments sequentially, collapsing and combining as you go\n",
    "    indices = np.arange(n_classes)\n",
    "    alignments = []\n",
    "    probabilities = []\n",
    "\n",
    "    for ind, probs in enumerate(prob_matrix):\n",
    "        \n",
    "        # Get the top 3\n",
    "        # previous_alignments adding - collapse at will - if the same as previous, don't add \n",
    "        # If new and the previous is blank, remove the blank\n",
    "        top_n = heapq.nlargest(3, enumerate(probs), key=lambda x: x[1])\n",
    "\n",
    "        alignments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([106, 1, 17])\n",
      "torch.Size([106, 17])\n",
      "[(0, tensor(0.)), (10, tensor(-19.0663)), (4, tensor(-19.7087))]\n",
      "[(0, tensor(-2.2530e-05)), (10, tensor(-10.6991)), (9, tensor(-17.3199))]\n",
      "[(0, tensor(0.)), (10, tensor(-18.4950)), (9, tensor(-23.0844))]\n",
      "[(0, tensor(-0.0006)), (10, tensor(-7.3804)), (9, tensor(-13.9946))]\n",
      "[(0, tensor(-1.4186e-05)), (10, tensor(-12.0249)), (5, tensor(-12.7543))]\n",
      "[(0, tensor(-0.0005)), (10, tensor(-7.5912)), (9, tensor(-11.7136))]\n",
      "[(0, tensor(0.)), (10, tensor(-19.5494)), (9, tensor(-20.2829))]\n",
      "[(0, tensor(-0.0004)), (10, tensor(-7.7915)), (9, tensor(-11.9784))]\n",
      "[(0, tensor(-3.9577e-05)), (8, tensor(-10.7000)), (7, tensor(-11.8956))]\n",
      "[(0, tensor(-0.0020)), (10, tensor(-6.2384)), (9, tensor(-11.6482))]\n",
      "[(0, tensor(0.)), (10, tensor(-20.0583)), (9, tensor(-22.7519))]\n",
      "[(0, tensor(-0.0003)), (11, tensor(-8.6603)), (10, tensor(-9.1101))]\n",
      "[(0, tensor(-0.0004)), (1, tensor(-8.5233)), (2, tensor(-8.8442))]\n",
      "[(0, tensor(-0.0035)), (11, tensor(-5.7383)), (10, tensor(-8.3454))]\n",
      "[(0, tensor(0.)), (10, tensor(-22.2299)), (5, tensor(-22.6592))]\n",
      "[(0, tensor(-0.0031)), (11, tensor(-5.8247)), (10, tensor(-9.1382))]\n",
      "[(0, tensor(-0.0051)), (4, tensor(-6.6540)), (5, tensor(-6.6553))]\n",
      "[(0, tensor(-0.0224)), (11, tensor(-3.8265)), (10, tensor(-7.8473))]\n",
      "[(0, tensor(0.)), (5, tensor(-19.8279)), (10, tensor(-20.6770))]\n",
      "[(0, tensor(-0.0209)), (11, tensor(-3.9024)), (10, tensor(-7.6281))]\n",
      "[(0, tensor(-0.0320)), (5, tensor(-4.4941)), (6, tensor(-4.6232))]\n",
      "[(0, tensor(-0.1073)), (11, tensor(-2.3026)), (10, tensor(-6.4034))]\n",
      "[(0, tensor(0.)), (7, tensor(-17.5765)), (6, tensor(-17.7420))]\n",
      "[(0, tensor(-0.1592)), (11, tensor(-1.9233)), (10, tensor(-7.0664))]\n",
      "[(0, tensor(-0.3042)), (8, tensor(-2.2614)), (7, tensor(-2.3151))]\n",
      "[(0, tensor(-0.4500)), (11, tensor(-1.0202)), (10, tensor(-6.4157))]\n",
      "[(0, tensor(-2.3842e-07)), (8, tensor(-16.4775)), (11, tensor(-17.0451))]\n",
      "[(12, tensor(-0.5990)), (0, tensor(-0.8625)), (11, tensor(-3.6276))]\n",
      "[(1, tensor(-0.8156)), (2, tensor(-1.5237)), (0, tensor(-1.8008))]\n",
      "[(12, tensor(-0.0557)), (0, tensor(-3.0315)), (11, tensor(-5.5187))]\n",
      "[(0, tensor(-3.5763e-06)), (3, tensor(-13.9208)), (5, tensor(-14.0677))]\n",
      "[(12, tensor(-0.0179)), (0, tensor(-4.2100)), (11, tensor(-6.5462))]\n",
      "[(3, tensor(-1.3599)), (4, tensor(-1.3970)), (5, tensor(-1.6203))]\n",
      "[(12, tensor(-0.0051)), (0, tensor(-5.7894)), (11, tensor(-7.1512))]\n",
      "[(0, tensor(-2.4557e-05)), (5, tensor(-11.6893)), (6, tensor(-11.8792))]\n",
      "[(12, tensor(-0.0038)), (0, tensor(-6.3948)), (13, tensor(-7.1379))]\n",
      "[(6, tensor(-1.0621)), (7, tensor(-1.4767)), (5, tensor(-1.6544))]\n",
      "[(12, tensor(-0.0011)), (13, tensor(-7.4919)), (11, tensor(-8.5059))]\n",
      "[(0, tensor(-0.0008)), (7, tensor(-8.0301)), (6, tensor(-8.7836))]\n",
      "[(12, tensor(-0.0017)), (13, tensor(-7.3252)), (14, tensor(-8.1072))]\n",
      "[(8, tensor(-0.5454)), (7, tensor(-1.1644)), (6, tensor(-2.6597))]\n",
      "[(12, tensor(-0.0043)), (13, tensor(-5.9929)), (10, tensor(-6.7153))]\n",
      "[(0, tensor(-0.0001)), (8, tensor(-10.6873)), (5, tensor(-10.8420))]\n",
      "[(13, tensor(-0.0035)), (12, tensor(-6.7248)), (14, tensor(-7.1482))]\n",
      "[(1, tensor(-0.6727)), (2, tensor(-1.2929)), (3, tensor(-2.1247))]\n",
      "[(13, tensor(-0.0047)), (11, tensor(-6.1460)), (12, tensor(-6.1879))]\n",
      "[(0, tensor(-0.0001)), (3, tensor(-10.2746)), (4, tensor(-10.4778))]\n",
      "[(13, tensor(-0.0032)), (11, tensor(-6.3409)), (14, tensor(-7.3145))]\n",
      "[(4, tensor(-1.1895)), (3, tensor(-1.3485)), (5, tensor(-1.7511))]\n",
      "[(13, tensor(-0.0031)), (11, tensor(-6.0301)), (14, tensor(-8.1153))]\n",
      "[(0, tensor(-0.0002)), (5, tensor(-9.7776)), (6, tensor(-9.9048))]\n",
      "[(13, tensor(-0.0022)), (11, tensor(-6.7228)), (14, tensor(-8.1701))]\n",
      "[(6, tensor(-1.1123)), (7, tensor(-1.1135)), (5, tensor(-1.6927))]\n",
      "[(13, tensor(-0.0009)), (11, tensor(-7.7964)), (14, tensor(-8.4516))]\n",
      "[(0, tensor(-0.0003)), (7, tensor(-8.9455)), (6, tensor(-9.2394))]\n",
      "[(13, tensor(-0.0023)), (14, tensor(-7.0066)), (11, tensor(-7.0369))]\n",
      "[(8, tensor(-0.4568)), (7, tensor(-1.4395)), (6, tensor(-2.2913))]\n",
      "[(13, tensor(-0.0023)), (14, tensor(-6.9532)), (11, tensor(-6.9606))]\n",
      "[(0, tensor(-5.4358e-05)), (8, tensor(-10.3974)), (6, tensor(-12.3029))]\n",
      "[(14, tensor(-0.0030)), (13, tensor(-6.1428)), (12, tensor(-7.3195))]\n",
      "[(1, tensor(-0.8327)), (2, tensor(-1.2681)), (3, tensor(-1.6470))]\n",
      "[(14, tensor(-0.0027)), (13, tensor(-6.4559)), (12, tensor(-6.8179))]\n",
      "[(0, tensor(-0.0001)), (14, tensor(-10.0880)), (4, tensor(-10.8201))]\n",
      "[(14, tensor(-0.0012)), (12, tensor(-7.2085)), (13, tensor(-7.9710))]\n",
      "[(4, tensor(-1.2535)), (3, tensor(-1.4103)), (5, tensor(-1.5383))]\n",
      "[(14, tensor(-0.0049)), (12, tensor(-5.5749)), (13, tensor(-6.8807))]\n",
      "[(0, tensor(-0.0001)), (6, tensor(-9.9808)), (5, tensor(-10.1780))]\n",
      "[(14, tensor(-0.0029)), (13, tensor(-6.5556)), (12, tensor(-6.6357))]\n",
      "[(6, tensor(-1.1243)), (5, tensor(-1.4057)), (7, tensor(-1.5210))]\n",
      "[(14, tensor(-0.0065)), (13, tensor(-5.3605)), (12, tensor(-6.4988))]\n",
      "[(0, tensor(-4.0650e-05)), (6, tensor(-11.1990)), (7, tensor(-11.3709))]\n",
      "[(14, tensor(-0.0075)), (13, tensor(-5.1943)), (12, tensor(-6.9062))]\n",
      "[(8, tensor(-0.5748)), (7, tensor(-1.2404)), (6, tensor(-2.2458))]\n",
      "[(14, tensor(-0.0050)), (13, tensor(-5.7571)), (12, tensor(-6.6213))]\n",
      "[(0, tensor(-2.1100e-05)), (15, tensor(-11.7658)), (8, tensor(-12.0060))]\n",
      "[(15, tensor(-0.0093)), (14, tensor(-5.0924)), (13, tensor(-6.0478))]\n",
      "[(1, tensor(-0.7303)), (2, tensor(-1.4691)), (3, tensor(-1.8487))]\n",
      "[(15, tensor(-0.0049)), (13, tensor(-5.9942)), (14, tensor(-6.1593))]\n",
      "[(0, tensor(-7.6053e-05)), (15, tensor(-9.9424)), (5, tensor(-11.7299))]\n",
      "[(15, tensor(-0.0062)), (14, tensor(-5.7596)), (13, tensor(-6.0367))]\n",
      "[(3, tensor(-1.3631)), (4, tensor(-1.3964)), (2, tensor(-1.5954))]\n",
      "[(15, tensor(-0.0074)), (14, tensor(-5.6754)), (13, tensor(-5.9359))]\n",
      "[(0, tensor(-1.6332e-05)), (15, tensor(-11.3696)), (6, tensor(-13.1049))]\n",
      "[(15, tensor(-0.0074)), (13, tensor(-5.6317)), (14, tensor(-6.2287))]\n",
      "[(5, tensor(-1.1843)), (6, tensor(-1.1916)), (7, tensor(-1.7380))]\n",
      "[(15, tensor(-0.0074)), (13, tensor(-5.3550)), (14, tensor(-6.0928))]\n",
      "[(0, tensor(-1.7166e-05)), (15, tensor(-11.7189)), (6, tensor(-12.5974))]\n",
      "[(15, tensor(-0.0085)), (14, tensor(-5.3573)), (13, tensor(-5.9920))]\n",
      "[(8, tensor(-0.5025)), (7, tensor(-1.5901)), (6, tensor(-2.0506))]\n",
      "[(15, tensor(-0.0068)), (14, tensor(-5.5096)), (13, tensor(-6.3142))]\n",
      "[(0, tensor(-6.6757e-06)), (15, tensor(-13.2479)), (1, tensor(-13.2851))]\n",
      "[(16, tensor(-0.0138)), (14, tensor(-5.2634)), (15, tensor(-5.3617))]\n",
      "[(1, tensor(-0.6691)), (2, tensor(-1.4748)), (3, tensor(-1.8255))]\n",
      "[(16, tensor(-0.0148)), (15, tensor(-5.2149)), (0, tensor(-5.3459))]\n",
      "[(0, tensor(-0.0002)), (2, tensor(-9.5066)), (3, tensor(-10.0152))]\n",
      "[(16, tensor(-0.0198)), (0, tensor(-4.4976)), (14, tensor(-5.3189))]\n",
      "[(3, tensor(-1.2698)), (4, tensor(-1.4465)), (2, tensor(-1.7347))]\n",
      "[(16, tensor(-0.0215)), (0, tensor(-4.2368)), (14, tensor(-5.7418))]\n",
      "[(0, tensor(-1.9550e-05)), (5, tensor(-12.0192)), (6, tensor(-12.1137))]\n",
      "[(16, tensor(-0.0143)), (0, tensor(-4.9353)), (15, tensor(-5.7948))]\n",
      "[(6, tensor(-1.0486)), (5, tensor(-1.5361)), (7, tensor(-1.6144))]\n",
      "[(16, tensor(-0.0155)), (0, tensor(-4.3735)), (15, tensor(-6.4839))]\n",
      "[(0, tensor(-0.0002)), (6, tensor(-9.8554)), (16, tensor(-10.0226))]\n",
      "[(16, tensor(-0.0201)), (0, tensor(-4.4608)), (15, tensor(-4.8148))]\n",
      "[(8, tensor(-0.7345)), (7, tensor(-1.2874)), (6, tensor(-1.7955))]\n",
      "[(16, tensor(-0.0165)), (0, tensor(-4.4656)), (15, tensor(-6.3796))]\n",
      "12 1 12 12 3 12 12 6 12 12 8 12 13 1 13 13 4 13 13 6 13 13 8 13 14 1 14 14 4 14 14 6 14 14 8 14 15 1 15 15 3 15 15 5 15 15 8 15 16 1 16 16 3 16 16 6 16 16 8 16\n",
      "None\n",
      "1 12 12 4 12 12 6 12 12 8 12 13 1 13 13 5 13 13 6 13 13 8 13 14 2 14 14 4 14 14 5 14 14 7 14 15 1 15 15 2 15 15 3 15 15 8 15 16 1 16 16 3 16 16 6 16 16 7 16\n",
      "0.9396825396825397\n",
      "0.0\n",
      "tensor(0.4244, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x_, y_ in zip(X, y):\n",
    "    #x = normalize([x_])\n",
    "    #input_sequence = normalize([x_], norm='l1')\n",
    "    input_sequence = torch.tensor(\n",
    "        x_, dtype=torch.float32)\n",
    "    input_sequence = input_sequence.view(1, 1, len(x_)).to(device)\n",
    "    model_output = model(input_sequence)\n",
    "    model_output = model_output.permute(1, 0, 2)\n",
    "    \n",
    "    label_lengths = torch.tensor([len(y_)])\n",
    "    target_sequence = torch.tensor(y_).to(device)\n",
    "\n",
    "    \n",
    "    n_timesteps = model_output.shape[0]\n",
    "    input_lengths = torch.tensor([n_timesteps])\n",
    "\n",
    "    print(model_output.shape)\n",
    "    \n",
    "    model_output_flattened = model_output.view(model_output.shape[0]* model_output.shape[1], n_classes)\n",
    "    print(model_output_flattened.shape)\n",
    "\n",
    "    loss = ctc(\n",
    "        model_output, target_sequence, input_lengths, label_lengths)\n",
    "    #print(loss.item())\n",
    "    \n",
    "    greedy_transcript = \" \".join(greedy_decoder(model_output))\n",
    "    beam_transcript = beam_search_ctc(model_output_flattened.detach().cpu())\n",
    "    actual_transcript = \" \".join([str(i) for i in y_])\n",
    "    print(greedy_transcript)\n",
    "    print(beam_transcript)\n",
    "    print(actual_transcript)\n",
    "    print(ratio(greedy_transcript, actual_transcript))\n",
    "    print(ratio(beam_transcript, actual_transcript))\n",
    "    print(loss)\n",
    "\n",
    "    break\n",
    "    \n",
    "    counter += 1\n",
    "    if counter == 2000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_config import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    n_classes=n_classes, hidden_size=256, window_size=1024, window_step=800, train_epochs=50, device=device,\n",
    "    model_save_path=\"\", write_path=\"\", dataset='synthetic', windows=True, sampling_rate=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_loop import run_epoch\n",
    "import torch.optim as optim\n",
    "from greedy_decoder import GreedyCTCDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "labels_int = np.arange(n_classes).tolist()\n",
    "labels = [f\"{i}\" for i in labels_int] # Tokens to be fed into greedy decoder\n",
    "greedy_decoder = GreedyCTCDecoder(labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2997/2997 [00:17<00:00, 167.28it/s]\n"
     ]
    }
   ],
   "source": [
    "result_dict = run_epoch(\n",
    "    model=model, model_config=model_config, X=X, y=y, ctc=ctc,\n",
    "    optimizer=optimizer, decoder=greedy_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.43492031, 3.12125254, 4.32518435, 3.63519311, 2.99334025,\n",
       "       4.46822262, 2.96295476, 4.26412487, 4.25731993, 3.65833068,\n",
       "       3.53381896, 3.62321472, 3.70630789, 3.79388523, 3.80692482,\n",
       "       4.41694069, 3.25016284, 4.02096653, 4.08079147, 2.94028783,\n",
       "       3.65108752, 3.69296646, 3.59262991, 3.48503113, 4.36539078,\n",
       "       3.61073542, 4.01794147, 4.51349545, 3.69478416, 4.07377005,\n",
       "       3.52363276, 3.65702391, 4.58049345, 3.65981936, 4.16589594,\n",
       "       4.36304283, 3.97712994, 3.81253338, 4.91858339, 3.19839787,\n",
       "       3.82459402, 3.97900391, 3.48522186, 4.26855755, 3.50788808,\n",
       "       3.45172596, 4.58332777, 3.93838859, 3.8704803 , 3.57057595,\n",
       "       3.34357715, 3.42299008, 3.44375992, 4.59503126, 3.65452838,\n",
       "       3.50559545, 3.79823709, 3.97137856, 3.20284557, 3.33833027,\n",
       "       4.01198483, 4.26743793, 4.20309877, 3.68024731, 2.64314175,\n",
       "       3.15585208, 3.95795512, 4.24880981, 4.16048098, 3.24434352,\n",
       "       3.70698881, 3.86576986, 3.04915285, 3.12248182, 2.59087253,\n",
       "       3.59172463, 4.4175148 , 3.17579818, 3.17413449, 3.64195108,\n",
       "       3.04405236, 4.00188637, 3.64966989, 4.17125225, 3.33976912,\n",
       "       3.54450655, 3.47780061, 2.23072886, 4.15369892, 4.28736305,\n",
       "       3.34872866, 2.87599015, 2.75624847, 4.2421999 , 3.64094758,\n",
       "       3.5190444 , 2.89819813, 4.27140617, 3.43613577, 4.29794455,\n",
       "       3.12363672, 3.19562888, 4.286654  , 3.44299722, 3.06735396,\n",
       "       3.36306548, 3.5420692 , 2.88930798, 3.68145156, 2.67626739,\n",
       "       3.43805718, 3.10154819, 4.13116169, 3.65994215, 3.94627857,\n",
       "       2.47408509, 2.70439219, 3.25199819, 4.68713665, 2.90851402,\n",
       "       4.16540575, 4.20670128, 2.66028786, 3.58072448, 3.97372031,\n",
       "       3.9984901 , 3.44481421, 3.20940924, 4.24665642, 3.34768677,\n",
       "       4.25253868, 3.65386081, 3.64637589, 4.73465729, 3.94411755,\n",
       "       3.76813293, 3.9928205 , 3.09398627, 4.41186523, 3.35268044,\n",
       "       3.47660089, 2.25387836, 4.43559885, 4.50788975, 3.66031551,\n",
       "       3.23696113, 3.97086048, 3.87714338, 3.33773851, 3.79836965,\n",
       "       3.61569524, 3.78526831, 2.65187097, 3.12541986, 3.64539957,\n",
       "       2.98349857, 4.48096466, 3.93772507, 4.55367756, 3.09045935,\n",
       "       3.8041749 , 3.80338192, 4.68282986, 3.86993575, 4.3492136 ,\n",
       "       3.61338758, 4.28675795, 4.14971209, 3.52621722, 3.43467903,\n",
       "       3.34363079, 3.05909657, 4.8108983 , 3.6508739 , 4.21107626,\n",
       "       3.86764431, 2.4555378 , 3.60036683, 4.23934174, 4.25497389,\n",
       "       3.59720826, 3.63817096, 4.57283783, 3.22958589, 3.72941256,\n",
       "       3.79785609, 4.37123966, 2.69851494, 4.38316393, 4.53457212,\n",
       "       3.61379957, 3.02984762, 2.64770484, 4.66755772, 3.27679658,\n",
       "       4.07742977, 3.94959402, 3.10719681, 4.13107061, 3.66442299,\n",
       "       4.28298044, 4.14276218, 4.15334368, 3.24837136, 4.26704693,\n",
       "       3.55991912, 3.65579295, 3.8709619 , 3.99878907, 3.23100591,\n",
       "       4.27841282, 2.68693972, 3.4742229 , 2.53028893, 3.49747849,\n",
       "       3.80148864, 3.63902164, 2.72783184, 4.26252699, 4.24207163,\n",
       "       4.3316741 , 3.52048039, 4.17141819, 3.84125876, 2.9727602 ,\n",
       "       3.11943126, 3.94693351, 3.5914588 , 3.87132144, 4.1788168 ,\n",
       "       2.92805004, 4.21560478, 3.66082549, 3.58685708, 4.10068178,\n",
       "       4.32056522, 3.37274003, 4.11955643, 4.06861496, 2.76857066,\n",
       "       3.41314936, 3.16121793, 3.76989675, 4.05547571, 3.64533591,\n",
       "       3.10717893, 3.62869644, 3.18711138, 4.21757507, 3.23920989,\n",
       "       3.509233  , 3.62262654, 3.71793675, 2.98810244, 4.10660553,\n",
       "       4.11648989, 3.86353588, 2.7274127 , 3.56607556, 3.80300474,\n",
       "       3.20419407, 4.44898415, 2.45106578, 3.17191172, 3.81697536,\n",
       "       3.78113198, 3.3987062 , 3.637779  , 3.17994952, 3.06723905,\n",
       "       4.10398912, 3.48473787, 3.96893382, 3.69105458, 3.24990249,\n",
       "       4.31617069, 3.80564141, 3.63008761, 3.39718413, 3.53361487,\n",
       "       4.00825596, 3.94957399, 3.67231202, 3.77798247, 4.22219038,\n",
       "       3.4658947 , 2.97552943, 4.32094383, 3.61152697, 3.59563875,\n",
       "       3.53918171, 3.59094667, 3.95292735, 3.5273025 , 3.51845217,\n",
       "       4.15603495, 3.86731625, 3.85674024, 4.58015728, 4.16616869,\n",
       "       3.79999113, 3.87666225, 3.20092297, 4.5687108 , 4.31228209,\n",
       "       3.42333269, 2.91362333, 3.66787434, 3.10607052, 3.48538494,\n",
       "       3.5299027 , 4.54372835, 4.43057013, 3.53760529, 3.04146957,\n",
       "       4.07233238, 3.01151848, 6.60128069, 3.17735505, 3.66347408,\n",
       "       3.67893362, 4.14090538, 2.80198073, 3.79027867, 3.56300116,\n",
       "       3.73443675, 4.24229574, 3.24749851, 4.16130924, 3.4504447 ,\n",
       "       3.81118321, 3.58502865, 3.35107517, 2.80555654, 3.1786232 ,\n",
       "       4.44636583, 3.96674204, 3.72292399, 4.27497578, 3.96469831,\n",
       "       3.50756431, 4.2122798 , 3.64980626, 3.48887181, 2.88758898,\n",
       "       4.23332024, 4.57293224, 3.20087814, 3.61595678, 3.58513331,\n",
       "       3.70006371, 4.16903877, 2.53241825, 2.85008526, 3.80309796,\n",
       "       3.98661232, 3.8333199 , 2.82774591, 4.32748175, 3.65123701,\n",
       "       4.18952417, 4.52532768, 4.07352018, 4.97710848, 4.37422895,\n",
       "       2.88523698, 3.09926605, 2.58583403, 3.69508362, 4.1222887 ,\n",
       "       3.43786502, 2.96213555, 4.43602371, 4.87695503, 3.08290315,\n",
       "       4.73323011, 3.79964137, 4.33024311, 4.1693058 , 3.50948882,\n",
       "       3.56012607, 3.05933189, 3.34369516, 3.43588901, 3.56273317,\n",
       "       3.65142965, 3.61392403, 4.09881306, 3.76655769, 3.09880781,\n",
       "       4.25460386, 3.43321538, 3.1913166 , 4.37056446, 2.68426466,\n",
       "       4.24463511, 4.83436346, 3.69382119, 3.66998792, 3.97495842,\n",
       "       3.10369086, 3.45117188, 3.43491745, 3.82109809, 4.58109713,\n",
       "       4.55292368, 3.4357543 , 3.04028893, 3.87351847, 2.2595098 ,\n",
       "       3.0349133 , 3.52946353, 4.02570057, 4.25907755, 4.23043871,\n",
       "       4.44834328, 3.43248153, 3.7819705 , 3.16535664, 3.90758157,\n",
       "       3.20290041, 3.76701975, 2.77202034, 3.93856907, 4.45926094,\n",
       "       3.81315613, 3.80037999, 3.08889794, 3.43754506, 3.43824673,\n",
       "       2.75523496, 4.47392368, 4.32327652, 3.58818436, 4.37364435,\n",
       "       4.41999722, 4.00084209, 3.4763782 , 3.34563804, 3.83821321,\n",
       "       4.17900276, 3.34474373, 4.01376915, 4.0596242 , 4.00235271,\n",
       "       3.83896112, 4.33851624, 3.24712396, 2.75110126, 4.05580425,\n",
       "       4.01448011, 3.47518086, 3.675282  , 4.34314585, 3.35561466,\n",
       "       3.84407353, 4.54878712, 2.63777018, 3.69330263, 3.35263085,\n",
       "       3.53153729, 3.20647502, 3.45064664, 3.58603168, 3.76831174,\n",
       "       4.37259674, 3.63651633, 4.21064472, 3.86376953, 4.1486268 ,\n",
       "       3.50904632, 2.97409511, 3.65881014, 3.96632409, 3.54527426,\n",
       "       4.00046873, 3.81194687, 4.12124252, 4.27460432, 3.77321911,\n",
       "       3.59307265, 3.39936781, 3.2089262 , 3.68933821, 4.47073412,\n",
       "       3.96041298, 3.20871305, 3.62606573, 3.49330759, 3.60972667,\n",
       "       3.25052476, 3.65259385, 3.09040046, 3.7243042 , 3.5634501 ,\n",
       "       3.61771846, 3.12968612, 4.69775295, 3.44433713, 3.45485497])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['losses'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sequence: [('01', np.float64(0.11616)), ('010', np.float64(0.07680000000000001))]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
