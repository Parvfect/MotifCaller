{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nn import MotifCaller, NaiveCaller\n",
    "from training_data import data_preproc, load_training_data\n",
    "from utils import get_savepaths\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from greedy_decoder import GreedyCTCDecoder\n",
    "from Levenshtein import ratio\n",
    "from beam_search_decoder import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 17\n",
    "model_path = r\"C:\\Users\\Parv\\Doc\\HelixWorks\\Basecalling\\code\\motifcaller\\models\\synthetic\\local_trained.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_int = np.arange(n_classes).tolist()\n",
    "labels = [f\"{i}\" for i in labels_int] # Tokens to be fed into greedy decoder\n",
    "greedy_decoder = GreedyCTCDecoder(labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Loading model purely for inference\n",
    "    Will need to lead optimizer to fine tune\n",
    "    \"\"\"\n",
    "    # Model Definition\n",
    "    model = NaiveCaller(num_classes=17)\n",
    "    \n",
    "    if device == torch.device('cpu'):\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "    \n",
    "\n",
    "# Load model\n",
    "# device\n",
    "# port these useful methods to utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path=model_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_savepaths(running_on_hpc=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>motif_seq</th>\n",
       "      <th>base_seq</th>\n",
       "      <th>squiggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 11, 11, 7, 11, 11, 8, 11, 12, 2, 12, 12, 5...</td>\n",
       "      <td>TTTATCGTCGTCACATCAGTCGACATCAGTCGGCATGAAGACACTA...</td>\n",
       "      <td>[504, 506, 491, 502, 504, 515, 458, 472, 458, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14, 14, 8, 14, 15, 2, 15, 15, 5, 15, 15, 7, 1...</td>\n",
       "      <td>TGACGTCGGATGACGTCGGCAGCGCCACCAACTCCACAAATGACGT...</td>\n",
       "      <td>[469, 480, 479, 479, 481, 482, 483, 498, 505, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10, 10, 3, 10, 10, 7, 10, 11, 2, 11, 11, 4, 1...</td>\n",
       "      <td>GGACAGCTAGGGACAGCTACACCCCCGTATTTTGAGCGGGGGACAG...</td>\n",
       "      <td>[465, 470, 479, 486, 539, 532, 533, 525, 527, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[12, 1, 12, 12, 4, 12, 12, 6, 12, 12, 7, 12, 1...</td>\n",
       "      <td>TCGCCTTCATACCCCACTAACGTAGAGTACTGCCCTTCATACCCCT...</td>\n",
       "      <td>[485, 487, 496, 495, 488, 488, 490, 488, 524, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 12, 3, 12, 12, 4, 12, 12, 6, 12, 12, 7, 1...</td>\n",
       "      <td>AGTCGCCTTCATACCCACCCCCGTATTTTGAGCGGCCTTCATACCC...</td>\n",
       "      <td>[544, 556, 539, 547, 536, 534, 538, 493, 488, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           motif_seq  \\\n",
       "0  [5, 11, 11, 7, 11, 11, 8, 11, 12, 2, 12, 12, 5...   \n",
       "1  [14, 14, 8, 14, 15, 2, 15, 15, 5, 15, 15, 7, 1...   \n",
       "2  [10, 10, 3, 10, 10, 7, 10, 11, 2, 11, 11, 4, 1...   \n",
       "3  [12, 1, 12, 12, 4, 12, 12, 6, 12, 12, 7, 12, 1...   \n",
       "4  [11, 12, 3, 12, 12, 4, 12, 12, 6, 12, 12, 7, 1...   \n",
       "\n",
       "                                            base_seq  \\\n",
       "0  TTTATCGTCGTCACATCAGTCGACATCAGTCGGCATGAAGACACTA...   \n",
       "1  TGACGTCGGATGACGTCGGCAGCGCCACCAACTCCACAAATGACGT...   \n",
       "2  GGACAGCTAGGGACAGCTACACCCCCGTATTTTGAGCGGGGGACAG...   \n",
       "3  TCGCCTTCATACCCCACTAACGTAGAGTACTGCCCTTCATACCCCT...   \n",
       "4  AGTCGCCTTCATACCCACCCCCGTATTTTGAGCGGCCTTCATACCC...   \n",
       "\n",
       "                                            squiggle  \n",
       "0  [504, 506, 491, 502, 504, 515, 458, 472, 458, ...  \n",
       "1  [469, 480, 479, 479, 481, 482, 483, 498, 505, ...  \n",
       "2  [465, 470, 479, 486, 539, 532, 533, 525, 527, ...  \n",
       "3  [485, 487, 496, 495, 488, 488, 490, 488, 524, ...  \n",
       "4  [544, 556, 539, 547, 536, 534, 538, 493, 488, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['motif_seq', 'base_seq', 'squiggle'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset_path, model_save_path, file_write_path = get_savepaths(\n",
    "        running_on_hpc=False)\n",
    "\n",
    "X, y = load_training_data(\n",
    "        dataset_path, column_x='squiggle', column_y='motif_seq',\n",
    "        sampling_rate=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import torch.nn as nn\n",
    "\n",
    "ctc = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "def update_alignments(alignments, alignment_probs, top_tokens, top_probs, beam_width=3, blank_index=0):\n",
    "\n",
    "    if len(alignments) == 0:\n",
    "        alignments.extend([[i] for i in top_tokens])\n",
    "        alignment_probs.extend(top_probs)\n",
    "        return alignments, alignment_probs\n",
    "\n",
    "\n",
    "    new_alignments = []\n",
    "    new_alignment_probs = []\n",
    "\n",
    "    for ind, alignment in enumerate(alignments):\n",
    "        last_char = alignment[-1]\n",
    "        for token, prob in zip(top_tokens, top_probs):\n",
    "            if token == last_char: # If it's the same as before (whether blank or repeated char - it gets collapsed)\n",
    "                new_alignment = alignment\n",
    "            elif last_char == blank_index:  # If previous is a blank and this is a character, we can get rid of the previous blank\n",
    "                new_alignment = alignment[:-1] + [token]\n",
    "            else:\n",
    "                new_alignment = alignment + [token]\n",
    "            \n",
    "            if new_alignment in new_alignments:\n",
    "                change_index = new_alignments.index(new_alignment)\n",
    "                old_prob = new_alignment_probs[\n",
    "                    change_index]\n",
    "                new_prob = np.log(np.exp(old_prob) + np.exp(alignment_probs[ind] + prob))\n",
    "                new_alignment_probs[\n",
    "                    change_index] += new_prob\n",
    "            else:\n",
    "                new_alignments.append(new_alignment)\n",
    "                new_alignment_probs.append(alignment_probs[ind] + prob)\n",
    "\n",
    "    # return the most probable one\n",
    "    # and then reduce to the beam width\n",
    "    # Sort new_alignment_probs in reverse order while preserving the relative order of new_alignments\n",
    "    sorted_pairs = sorted(zip(new_alignment_probs, new_alignments), key=lambda x: -x[0])\n",
    "\n",
    "    # Unzip the sorted result\n",
    "    new_alignment_probs, new_alignments = zip(*sorted_pairs)\n",
    "\n",
    "    # Convert back to lists if needed\n",
    "    new_alignment_probs = list(new_alignment_probs)\n",
    "    new_alignments = list(new_alignments)\n",
    "\n",
    "    return new_alignments[:beam_width], new_alignment_probs[:beam_width]\n",
    "\n",
    "\n",
    "def beam_search_ctc(prob_matrix, beam_width=3, blank=0, n_classes=17):\n",
    "    \n",
    "    # Get top n probabilities and their corresponding indices for each time step\n",
    "    # Create a list of alignments sequentially, collapsing and combining as you go\n",
    "    indices = np.arange(n_classes)\n",
    "    alignments, alignment_probs = [], []\n",
    "    for ind, probs in enumerate(prob_matrix):\n",
    "        # Get the top 3\n",
    "        # previous_alignments adding - collapse at will - if the same as previous, don't add \n",
    "        # If new and the previous is blank, remove the blank\n",
    "        top_n = heapq.nlargest(n_classes, enumerate(probs), key=lambda x: x[1])\n",
    "        top_tokens = [i[0] for i in top_n]\n",
    "        top_probs = [float(i[1]) for i in top_n]\n",
    "        alignments, alignment_probs = update_alignments(\n",
    "            alignments, alignment_probs, top_tokens, top_probs,\n",
    "            beam_width=beam_width)\n",
    "\n",
    "    return \" \".join([str(i) for i in alignments[0]])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cb6351055149ff894526e7673626bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01923835680445307\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "counter = 0\n",
    "sum_diff = 0\n",
    "for x_, y_ in tqdm(zip(X, y), total=len(X)):\n",
    "    #x = normalize([x_])\n",
    "    #input_sequence = normalize([x_], norm='l1')\n",
    "    input_sequence = torch.tensor(\n",
    "        x_, dtype=torch.float32)\n",
    "    input_sequence = input_sequence.view(1, 1, len(x_)).to(device)\n",
    "    model_output = model(input_sequence)\n",
    "    model_output = model_output.permute(1, 0, 2)\n",
    "    \n",
    "    label_lengths = torch.tensor([len(y_)])\n",
    "    target_sequence = torch.tensor(y_).to(device)\n",
    "\n",
    "    \n",
    "    n_timesteps = model_output.shape[0]\n",
    "    input_lengths = torch.tensor([n_timesteps])\n",
    "\n",
    "    \n",
    "    model_output_flattened = model_output.view(model_output.shape[0]* model_output.shape[1], n_classes)\n",
    "\n",
    "    loss = ctc(\n",
    "        model_output, target_sequence, input_lengths, label_lengths)\n",
    "    #print(loss.item())\n",
    "    \n",
    "    greedy_transcript = \" \".join(greedy_decoder(model_output))\n",
    "    beam_transcript = beam_search_ctc(model_output_flattened.detach().cpu(), beam_width=10)\n",
    "    actual_transcript = \" \".join([str(i) for i in y_])\n",
    "    \n",
    "    greedy_ratio = ratio(greedy_transcript, actual_transcript)\n",
    "    beam_ratio = ratio(beam_transcript, actual_transcript)\n",
    "    sum_diff += beam_ratio - greedy_ratio\n",
    "\n",
    "    counter += 1\n",
    "    if counter == 100:\n",
    "        break\n",
    "\n",
    "print(sum_diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_config import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    n_classes=n_classes, hidden_size=256, window_size=1024, window_step=800, train_epochs=50, device=device,\n",
    "    model_save_path=\"\", write_path=\"\", dataset='synthetic', windows=True, sampling_rate=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_loop import run_epoch\n",
    "import torch.optim as optim\n",
    "from greedy_decoder import GreedyCTCDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "labels_int = np.arange(n_classes).tolist()\n",
    "labels = [f\"{i}\" for i in labels_int] # Tokens to be fed into greedy decoder\n",
    "greedy_decoder = GreedyCTCDecoder(labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2997/2997 [00:17<00:00, 167.28it/s]\n"
     ]
    }
   ],
   "source": [
    "result_dict = run_epoch(\n",
    "    model=model, model_config=model_config, X=X, y=y, ctc=ctc,\n",
    "    optimizer=optimizer, decoder=greedy_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.43492031, 3.12125254, 4.32518435, 3.63519311, 2.99334025,\n",
       "       4.46822262, 2.96295476, 4.26412487, 4.25731993, 3.65833068,\n",
       "       3.53381896, 3.62321472, 3.70630789, 3.79388523, 3.80692482,\n",
       "       4.41694069, 3.25016284, 4.02096653, 4.08079147, 2.94028783,\n",
       "       3.65108752, 3.69296646, 3.59262991, 3.48503113, 4.36539078,\n",
       "       3.61073542, 4.01794147, 4.51349545, 3.69478416, 4.07377005,\n",
       "       3.52363276, 3.65702391, 4.58049345, 3.65981936, 4.16589594,\n",
       "       4.36304283, 3.97712994, 3.81253338, 4.91858339, 3.19839787,\n",
       "       3.82459402, 3.97900391, 3.48522186, 4.26855755, 3.50788808,\n",
       "       3.45172596, 4.58332777, 3.93838859, 3.8704803 , 3.57057595,\n",
       "       3.34357715, 3.42299008, 3.44375992, 4.59503126, 3.65452838,\n",
       "       3.50559545, 3.79823709, 3.97137856, 3.20284557, 3.33833027,\n",
       "       4.01198483, 4.26743793, 4.20309877, 3.68024731, 2.64314175,\n",
       "       3.15585208, 3.95795512, 4.24880981, 4.16048098, 3.24434352,\n",
       "       3.70698881, 3.86576986, 3.04915285, 3.12248182, 2.59087253,\n",
       "       3.59172463, 4.4175148 , 3.17579818, 3.17413449, 3.64195108,\n",
       "       3.04405236, 4.00188637, 3.64966989, 4.17125225, 3.33976912,\n",
       "       3.54450655, 3.47780061, 2.23072886, 4.15369892, 4.28736305,\n",
       "       3.34872866, 2.87599015, 2.75624847, 4.2421999 , 3.64094758,\n",
       "       3.5190444 , 2.89819813, 4.27140617, 3.43613577, 4.29794455,\n",
       "       3.12363672, 3.19562888, 4.286654  , 3.44299722, 3.06735396,\n",
       "       3.36306548, 3.5420692 , 2.88930798, 3.68145156, 2.67626739,\n",
       "       3.43805718, 3.10154819, 4.13116169, 3.65994215, 3.94627857,\n",
       "       2.47408509, 2.70439219, 3.25199819, 4.68713665, 2.90851402,\n",
       "       4.16540575, 4.20670128, 2.66028786, 3.58072448, 3.97372031,\n",
       "       3.9984901 , 3.44481421, 3.20940924, 4.24665642, 3.34768677,\n",
       "       4.25253868, 3.65386081, 3.64637589, 4.73465729, 3.94411755,\n",
       "       3.76813293, 3.9928205 , 3.09398627, 4.41186523, 3.35268044,\n",
       "       3.47660089, 2.25387836, 4.43559885, 4.50788975, 3.66031551,\n",
       "       3.23696113, 3.97086048, 3.87714338, 3.33773851, 3.79836965,\n",
       "       3.61569524, 3.78526831, 2.65187097, 3.12541986, 3.64539957,\n",
       "       2.98349857, 4.48096466, 3.93772507, 4.55367756, 3.09045935,\n",
       "       3.8041749 , 3.80338192, 4.68282986, 3.86993575, 4.3492136 ,\n",
       "       3.61338758, 4.28675795, 4.14971209, 3.52621722, 3.43467903,\n",
       "       3.34363079, 3.05909657, 4.8108983 , 3.6508739 , 4.21107626,\n",
       "       3.86764431, 2.4555378 , 3.60036683, 4.23934174, 4.25497389,\n",
       "       3.59720826, 3.63817096, 4.57283783, 3.22958589, 3.72941256,\n",
       "       3.79785609, 4.37123966, 2.69851494, 4.38316393, 4.53457212,\n",
       "       3.61379957, 3.02984762, 2.64770484, 4.66755772, 3.27679658,\n",
       "       4.07742977, 3.94959402, 3.10719681, 4.13107061, 3.66442299,\n",
       "       4.28298044, 4.14276218, 4.15334368, 3.24837136, 4.26704693,\n",
       "       3.55991912, 3.65579295, 3.8709619 , 3.99878907, 3.23100591,\n",
       "       4.27841282, 2.68693972, 3.4742229 , 2.53028893, 3.49747849,\n",
       "       3.80148864, 3.63902164, 2.72783184, 4.26252699, 4.24207163,\n",
       "       4.3316741 , 3.52048039, 4.17141819, 3.84125876, 2.9727602 ,\n",
       "       3.11943126, 3.94693351, 3.5914588 , 3.87132144, 4.1788168 ,\n",
       "       2.92805004, 4.21560478, 3.66082549, 3.58685708, 4.10068178,\n",
       "       4.32056522, 3.37274003, 4.11955643, 4.06861496, 2.76857066,\n",
       "       3.41314936, 3.16121793, 3.76989675, 4.05547571, 3.64533591,\n",
       "       3.10717893, 3.62869644, 3.18711138, 4.21757507, 3.23920989,\n",
       "       3.509233  , 3.62262654, 3.71793675, 2.98810244, 4.10660553,\n",
       "       4.11648989, 3.86353588, 2.7274127 , 3.56607556, 3.80300474,\n",
       "       3.20419407, 4.44898415, 2.45106578, 3.17191172, 3.81697536,\n",
       "       3.78113198, 3.3987062 , 3.637779  , 3.17994952, 3.06723905,\n",
       "       4.10398912, 3.48473787, 3.96893382, 3.69105458, 3.24990249,\n",
       "       4.31617069, 3.80564141, 3.63008761, 3.39718413, 3.53361487,\n",
       "       4.00825596, 3.94957399, 3.67231202, 3.77798247, 4.22219038,\n",
       "       3.4658947 , 2.97552943, 4.32094383, 3.61152697, 3.59563875,\n",
       "       3.53918171, 3.59094667, 3.95292735, 3.5273025 , 3.51845217,\n",
       "       4.15603495, 3.86731625, 3.85674024, 4.58015728, 4.16616869,\n",
       "       3.79999113, 3.87666225, 3.20092297, 4.5687108 , 4.31228209,\n",
       "       3.42333269, 2.91362333, 3.66787434, 3.10607052, 3.48538494,\n",
       "       3.5299027 , 4.54372835, 4.43057013, 3.53760529, 3.04146957,\n",
       "       4.07233238, 3.01151848, 6.60128069, 3.17735505, 3.66347408,\n",
       "       3.67893362, 4.14090538, 2.80198073, 3.79027867, 3.56300116,\n",
       "       3.73443675, 4.24229574, 3.24749851, 4.16130924, 3.4504447 ,\n",
       "       3.81118321, 3.58502865, 3.35107517, 2.80555654, 3.1786232 ,\n",
       "       4.44636583, 3.96674204, 3.72292399, 4.27497578, 3.96469831,\n",
       "       3.50756431, 4.2122798 , 3.64980626, 3.48887181, 2.88758898,\n",
       "       4.23332024, 4.57293224, 3.20087814, 3.61595678, 3.58513331,\n",
       "       3.70006371, 4.16903877, 2.53241825, 2.85008526, 3.80309796,\n",
       "       3.98661232, 3.8333199 , 2.82774591, 4.32748175, 3.65123701,\n",
       "       4.18952417, 4.52532768, 4.07352018, 4.97710848, 4.37422895,\n",
       "       2.88523698, 3.09926605, 2.58583403, 3.69508362, 4.1222887 ,\n",
       "       3.43786502, 2.96213555, 4.43602371, 4.87695503, 3.08290315,\n",
       "       4.73323011, 3.79964137, 4.33024311, 4.1693058 , 3.50948882,\n",
       "       3.56012607, 3.05933189, 3.34369516, 3.43588901, 3.56273317,\n",
       "       3.65142965, 3.61392403, 4.09881306, 3.76655769, 3.09880781,\n",
       "       4.25460386, 3.43321538, 3.1913166 , 4.37056446, 2.68426466,\n",
       "       4.24463511, 4.83436346, 3.69382119, 3.66998792, 3.97495842,\n",
       "       3.10369086, 3.45117188, 3.43491745, 3.82109809, 4.58109713,\n",
       "       4.55292368, 3.4357543 , 3.04028893, 3.87351847, 2.2595098 ,\n",
       "       3.0349133 , 3.52946353, 4.02570057, 4.25907755, 4.23043871,\n",
       "       4.44834328, 3.43248153, 3.7819705 , 3.16535664, 3.90758157,\n",
       "       3.20290041, 3.76701975, 2.77202034, 3.93856907, 4.45926094,\n",
       "       3.81315613, 3.80037999, 3.08889794, 3.43754506, 3.43824673,\n",
       "       2.75523496, 4.47392368, 4.32327652, 3.58818436, 4.37364435,\n",
       "       4.41999722, 4.00084209, 3.4763782 , 3.34563804, 3.83821321,\n",
       "       4.17900276, 3.34474373, 4.01376915, 4.0596242 , 4.00235271,\n",
       "       3.83896112, 4.33851624, 3.24712396, 2.75110126, 4.05580425,\n",
       "       4.01448011, 3.47518086, 3.675282  , 4.34314585, 3.35561466,\n",
       "       3.84407353, 4.54878712, 2.63777018, 3.69330263, 3.35263085,\n",
       "       3.53153729, 3.20647502, 3.45064664, 3.58603168, 3.76831174,\n",
       "       4.37259674, 3.63651633, 4.21064472, 3.86376953, 4.1486268 ,\n",
       "       3.50904632, 2.97409511, 3.65881014, 3.96632409, 3.54527426,\n",
       "       4.00046873, 3.81194687, 4.12124252, 4.27460432, 3.77321911,\n",
       "       3.59307265, 3.39936781, 3.2089262 , 3.68933821, 4.47073412,\n",
       "       3.96041298, 3.20871305, 3.62606573, 3.49330759, 3.60972667,\n",
       "       3.25052476, 3.65259385, 3.09040046, 3.7243042 , 3.5634501 ,\n",
       "       3.61771846, 3.12968612, 4.69775295, 3.44433713, 3.45485497])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['losses'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sequence: [('01', np.float64(0.11616)), ('010', np.float64(0.07680000000000001))]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
